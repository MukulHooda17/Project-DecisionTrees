# Project-DecisionTrees
Project is divided in 2 parts -
1. Print the Decision tree.
2. Decision Tree Implementation:  Building actual decision tree for the dataset and then printing it.

Implementation: Consider the decision tree for OR below:
Level  0
Count of  0(False)  =  1
Count of  1(True)  =  3
Current Entropy  is =  0.811278124459
Splitting on feature  X1  with gain ratio  0.311278124459

Level  1
Count of  0  =  1
Count of  1  =  1
Current Entropy is =  1.0
Splitting on feature  X2  with gain ratio  1.0

Level  2
Count of  0  =  1
Current Entropy  is =  0.0
Reached leaf Node

Level  2
Count of  1  =  1
Current Entropy  is =  0.0
Reached leaf Node

Level  1
Count of  1  =  2
Current Entropy  is =  0.0
Reached leaf Node
